# -*- coding: utf-8 -*-
"""Chemical.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HLn48fOgsVTzkQNhDOXoFA_-KGpzo5hs
"""

import pandas as pd 
import numpy as np
import seaborn as sns; sns.set()
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import RFE
from sklearn.svm import SVR
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

import os
os.listdir('/content/drive/My Drive/DataSet')

data = pd.read_csv("/content/drive/My Drive/DataSet/musk_csv.csv")
print(data.columns)

data.isna().any()

z=len(data['class'])
c1=0
c2=0
for i in range(z):
    if data['class'][i]==1:
        c1+=1
    else:
        c2+=1

print("Total 0:",c1)
print("Total 1:",c2)

total_rows=len(data.axes[0])
total_cols=len(data.axes[1])
print("Number of Rows: "+str(total_rows))
print("Number of Columns: "+str(total_cols))

features = data.iloc[:,4:-1]  #independent columns
labels =   data.iloc[:,-1]

features_imp = data.iloc[:,10:20]  #independent column 
total_rows=len(features_imp.axes[0])
total_cols=len(features_imp.axes[1])

print("Number of Rows: "+str(total_rows))
print("Number of Columns: "+str(total_cols))
horizontal_stack = pd.concat([features_imp, labels], axis=1)
print(horizontal_stack.columns)

f,ax = plt.subplots(figsize=(12,10))
sns.heatmap(horizontal_stack.corr(),annot=True, linewidths=.1, fmt='.1f', ax=ax)
plt.show()

# Create correlation matrix
corr_matrix = features.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than or eaual to 0.7
to_drop = [column for column in upper.columns if any(upper[column] >= 0.7)]

# Drop features 
features.drop(to_drop, axis=1, inplace=True)

print(features.columns)

print(len(features.axes[0]))
print(len(features.axes[1]))

import pandas as pd
import numpy as np
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(features,labels)
#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=features.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

estimator = SVR(kernel="linear")
selector = RFE(estimator, 10, step=1)
selector = selector.fit(features_imp,labels)
print(selector.support_) 
print(selector.ranking_)

from sklearn.preprocessing import scale

X_train, y_test, X_labels, y_labels = train_test_split(features,
                                                          labels,
                                                          test_size=0.20,
                                                          random_state=42)
X_train=scale(X_train)
y_test=scale(y_test)

X_train.shape[0]

from keras.models import Sequential
from keras.layers import Dense
import keras
model = Sequential()
model.add(Dense(12, input_dim=54, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# compile the keras model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
callback_early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')

import keras
from matplotlib import pyplot as plt
history = model.fit(X_train,  X_labels,validation_split = 0.1, epochs=150, batch_size=4)
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

y_pred = model.predict(y_test)

print(len(y_pred))

y_pred

for i in range(len(y_pred)):
  if(y_pred[i]>0.5):
    y_pred[i]=1
  else:
    y_pred[i]=0

print("prediciton accuracy:",accuracy_score(y_labels,y_pred))
print("f1_Score:",f1_score(y_labels,y_pred, average="macro"))
print("precision_score:",precision_score(y_labels,y_pred, average="macro"))
print("recall_score:",recall_score(y_labels,y_pred, average="macro"))
tneg,fpos,fneg,tpos=confusion_matrix(y_labels,y_pred).ravel()
print("tneg:{},fpos:{},fneg:{},tpos:{}".format(tneg,fpos,fneg,tpos))
Senstivity=tpos/(tpos+fneg)
print("senstivity:{}".format(Senstivity))

model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")
print("Saved model to disk")

